from .chunk_utils import expand_chunks

from typing import Tuple, Generator
from numpy.typing import ArrayLike
import logging
import dask.array as da
import s3fs, zarr
import ray
import numpy as np


_LOGGER = logging.getLogger(__name__)

class BlockedArrayWriter:
    @staticmethod
    def gen_slices(
        arr_shape: Tuple[int, ...], block_shape: Tuple[int, ...]
    ) -> Generator:
        """
        Generate a series of slices that can be used to traverse an array in blocks of a given shape.

        The method generates tuples of slices, each representing a block of the array. The blocks are generated by
        iterating over the array in steps of the block shape along each dimension.

        Parameters
        ----------
        arr_shape : tuple of int
            The shape of the array to be sliced.

        block_shape : tuple of int
            The desired shape of the blocks. This should be a tuple of integers representing the size of each
            dimension of the block. The length of `block_shape` should be equal to the length of
            `arr_shape`. If the array shape is not divisible by the block shape along a dimension, the last slice
            along that dimension is truncated.

        Returns
        -------
        generator of tuple of slice
            A generator yielding tuples of slices. Each tuple can be used to index the input array.
        """
        if len(arr_shape) != len(block_shape):
            raise Exception(
                "array shape and block shape have different lengths"
            )

        def _slice_along_dim(dim: int) -> Generator:
            """A helper generator function that slices along one dimension."""
            # Base case: if the dimension is beyond the last one, return an empty tuple
            if dim >= len(arr_shape):
                yield ()
            else:
                # Iterate over the current dimension in steps of the block size
                for i in range(0, arr_shape[dim], block_shape[dim]):
                    # Calculate the end index for this block
                    end_i = min(i + block_shape[dim], arr_shape[dim])
                    # Generate slices for the remaining dimensions
                    for rest in _slice_along_dim(dim + 1):
                        yield (slice(i, end_i),) + rest

        # Start slicing along the first dimension
        return _slice_along_dim(0)

    @staticmethod
    def store(
        in_array: da.Array, out_array: ArrayLike, block_shape: tuple, write_root: str, 
        write_ds: str, input_path
    ) -> None:
        """
        Partitions the last 3 dimensions of a Dask array into non-overlapping blocks and
        writes them sequentially to a Zarr array. This is meant to reduce the scheduling burden
        for massive (terabyte-scale) arrays.

        :param in_array: The input Dask array
        :param block_shape: Tuple of (block_depth, block_height, block_width)
        :param out_array: The output array
        """
        
        @ray.remote(num_cpus=1)
        def process_multiscale_task(sl, input_path, write_root, write_ds) -> tuple:
            s3r = s3fs.S3FileSystem(anon=False)
            src = zarr.open(s3fs.S3Map(root=input_path, s3=s3r), mode="r")[write_ds]
            print("read done")

            s3w = s3fs.S3FileSystem(anon=False)
            dst = zarr.open(s3fs.S3Map(root=write_root, s3=s3w), mode="a")[write_ds]

            block = src[sl]                            
            dst[sl] = np.ascontiguousarray(block)  
            print("write done")     
            return None

        ray.init()

        futures = [
            process_multiscale_task.remote(sl, input_path, write_root, write_ds)
            for sl in BlockedArrayWriter.gen_slices(in_array.shape, block_shape)
        ]

        results = ray.get(futures)
        

    @staticmethod
    def get_block_shape(arr, target_size_mb=1024, mode="cycle"):
        """
        Given the shape and chunk size of a pre-chunked array, determine the optimal block shape
        closest to target_size. Expanded block dimensions are an integer multiple of the chunk dimension
        to ensure optimal access patterns.
        Args:
            arr: the input array
            target_size_mb: target block size in megabytes, default is 409600
            mode: strategy. Must be one of "cycle", or "iso"
        Returns:
            the block shape
        """
        if isinstance(arr, da.Array):
            chunks = arr.chunksize[-3:]
        else:
            chunks = arr.chunks[-3:]

        return expand_chunks(
            chunks,
            arr.shape[-3:],
            target_size_mb * 1024**2,
            arr.itemsize,
            mode,
        )